---
title: "Feb_25_&_27"
format: html
editor: visual
---

## Confidence Intervals

the standard error is a measure of uncertainty in a statistic value the CI

the CI is another way of describing a statistics sampling distribution.

Interval around an estimate around a statistic we generate.

mean+\_critiacl value x SE of the mean

the value of the statistic +- some critical value x the standard error of the statistic

## Null Hypothesis Testing

**Null Hypothesis; H~0~ : NO IMPACT**

-   No Deviation for what is expected

-   baseline hypothesis and is the claim that is presumed to be true. That claim is typically that a particular value of a population parameter estimated by a sample statistic we have calculated is consistent with a praticular null expectation. The alternative hypothesis is the conjecture that we are testing.

**Alternative Hypothesis; H~[A]{.smallcaps}~:** There is a difference in treatment.

-   Deviates more than expected by chance from what is expected or neutral

1.  **Two Tailed**: any difference in time between two chambers

2.  **One Tailed**: mean time spend in a particular chamber is greater (less than) in alternative

[To implement a hypothesis test, we need to]{.underline}

1.  Calcualte a test statistic

2.  calculate the p value

3.  compare the test statistic to some appropriate standardized sampling distribution with well-known mathematical properties. to yield a

4.  evaluate whether the p value is less than or greater than the significance level, or a, that we set for out test

    1.  a can be though to as the cutoff level for p values

5.  In traditional parametric statistics, then we make the assumption the sampling distribution o four statistic of interest. takes the form of a particular well - unerstood mathematical distribution (e.g. the normal).

## Process: Example from Module 15: One Sample T Test

```{r}
library(tidyverse)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/woolly-weights.csv"
d<-read_csv(f, col_names = TRUE)
head(d)
```

Calculate the mean, standard deviation, and SE of your samples

```{r}
m<- mean(d$weight)
s<- sd(d$weight)
se <- s/sqrt(length(d$weight))
n<-length(d$weight)
se<-s/sqrt(n)
```

Test Statistics = m= mean of our sample mu= null hypothesis (expectation)

```{r}
mu <- 7.2 #(null)
t_stat <- (m - mu)/se
t_stat
```

according to our T_stat = -3.3 we are -3.3 (smaller) standard error units away from our expectation

A T-statistic is a measure of how far your statistic is from your expectation

### Calculating the 95% CI around your estimate of the mean 6.4

1.  **standard normal (qnorm)**

```{r}
ci <- m + qnorm(c(0.025, 0.975)) * se
ci
```

2.  **T-distribution (qt) (fatter; sample size is less than 30)**

```{r}
ci<-m + qt(p=c(0.025,0.975), ncp = 0, df = n-1) *se
ci
```

3.  B**oot Strapping**

    we are sampling with replacement; sampling 15 observations but allow them to be replaced each time

    Does not include expected (null hypothesis (7.2))

    expected value of *statistic* = 0

```{r}
n_boot <- 10000
boot <- do(n_boot)*mean(sample(d$weight), n, replace = TRUE) 
boot

```

```{r}
n_boot <- 10000
boot<- vector()
for (i in 1:n_boot){
  boot[[i]] <- mean(sample(d$weight, n, replace = TRUE))
}
head(boot)
hist(boot)
ci <- quantile(probs = c(0.025, 0.975), boot)
ci
```

What is the two tailed probability

```{r}
p_lower <-pt(-1*abs(t_stat), df = n-1)
p_upper <-pt(1* abs(t_stat), df= n-1)
p <-p_lower + p_upper
```

T-test function in R that will do all of this for us

```{r}
t.test(x = d$weight, mu = mu , alternative = "two.sided")
```

### END OF CLASS CHALLENGE

```{r}
f <-"https://raw.githubusercontent.com/difiore/ada-datasets/main/tbs-2006-2008-ranges.csv"
d<-read_csv(f, col_names = TRUE)
head(d)
```

```{r}
stats <-d |>
  group_by(sex)|>
  summarize(mk95 = mean(kernel95),
            sdk95 = sd(kernel95),
            sek95 = sd(kernel95)/sqrt(length(kernel95)))
show(stats)
```

Creating a boxplot

```{r}
p <- ggplot(data = d, mapping = aes(x = sex, y = kernel95))+
  geom_boxplot()+
  geom_point()
p
```

## Class Feb 27

### A Two Sample T-test

```{r}
f <-"https://raw.githubusercontent.com/difiore/ada-datasets/main/tbs-2006-2008-ranges.csv"
d <- read_csv(f, col_names = TRUE)
d 
head(d)
```

We are interested in the variables "sex" and "Kernel95" single Sample of home range sizes for indivuidal spider monkeys from one social group in ecuador

For each sex, generate a bootstrap distribution from the mean of home rangesize, where you resample the data within each sex 10,000 times

```{r}
library(dplyr)
library(boot)
males <- d %>% filter(sex== "M")
females <- d %>% filter(sex == "F")



```

```{r}
library(dplyr)
library(boot)
males <- d %>% filter(sex== "M")
females <- d %>% filter(sex == "F")

male_mean <- males %>% 
  summarize(mean_kernel95 = mean(kernel95, na.rm = TRUE))


female_mean <- females %>%
  summarize(mean_kernel95 = mean(kernel95, na.rm = TRUE))


print(male_mean)
print(female_mean)
```

```{r}
#Bootstrap for males
n_boot <- 10000
male_boot_mean <- numeric(n_boot)
female_boot_mean <- numeric (n_boot)

 
for(i in 1:n_boot){
  male_boot_mean[i] <- mean(sample(males$kernel95, 
                                   size = nrow(males), 
                                   replace = TRUE), 
                            na.rm = TRUE)
}

hist(male_boot_mean, main = "Bootstrap Distribution of Male Mean kernel95",
     xlab = "Mean kernel95", col = "lightblue")

#Bootstrap for females
for(i in 1:n_boot){
  female_boot_mean[i] <- mean(sample(females$kernel95, 
                                     size = nrow(females), 
                                     replace = TRUE), 
                              na.rm = TRUE)
}
hist(female_boot_mean, main="Bootstrap Distribution of female Mean kernel95",
     xlab = "Mean kernel95", col = "lightblue")

```

Is there a signitficant difference in mean hoome range size of males and females ? what is Ho what is Ha Two sample t-test assuming roughly equal varaince int he two groups what is the test statistic (see module 15) (welches t-statistic)

### **Samples with Equal Variances**

### ![](images/Screenshot%202025-02-27%20at%204.16.09%20PM-01.png){width="242"}

### Thereâ€™s a simpler T statistic we can use if the variances of the two samples are more or less equal.

![](images/Screenshot%202025-02-27%20at%204.18.38%20PM-01.png)

```{r}
n <- (318.7846- 429.7407-0)
n
```

```{r}
s2 <- ((nrow(males) - 1) * sd(males$kernel95)^2 + 
       (nrow(females) - 1) * sd(females$kernel95)^2)/(nrow(males) + nrow(females) - 2)
t_stat <-  t_stat <- (mean(males$kernel95) - mean(females$kernel95)) / 
          sqrt(s2 * (1/nrow(males) + 1/nrow(females)))
df <- nrow(males) + nrow(females) - 2
p_value <- 2 * pt(-abs(t_stat), df)
print(paste("t-statistic:", t_stat))
print(paste("degrees of freedom:", df))
print(paste("p-value:", p_value))


```

## R's built-in t.test function, which handles all these calculations for you:

```{r}
result <- t.test(males$kernel95, females$kernel95, var.equal = TRUE)
print(result)
```

## Programming out own two-sample permutation test alternative to this standard parametric t-test

1.  Choose a test statistic

-   Use the difference between means of the two groups

2.  Define the null hypothesis

-   Two-tailed: The mean for males and females is the same

-   or

-   One-tailed: The mean for one sex is greater/less than that for the other sex

3.  Generate a permutation distribution

-   Create distribution for the test statistic by repeatedly shuffling group labels

-   This requires ***permutation*** to break the association we want to test (in this case, between sex and home range size)

4.  Calculate the p-value

-   Determine the probability of observing a test statistic as extreme as the one we actually got

-   This tells us how likely our result would be if the null hypothesis of no difference were true

```{r}
#permutation Test
d <-d |>
  select(id, sex, kernel95)

library(dplyr)

# Create summary statistics
summary <- d |>
  group_by(sex) |>
  summarize(mean = mean(kernel95))  # Name the column explicitly

# Calculate observed difference
obs <- filter(summary, sex == "F") |> pull(mean) -
      filter(summary, sex == "M") |> pull(mean)

# Prepare for permutation test
reps <- 10000
perm <- vector(length = reps)
  

for(i in 1:reps){
  temp <- d
  temp$sex <- sample(temp$sex)
  summary <- temp |>
    group_by(sex) |>
    summarize(mean = mean(kernel95, na.rm = TRUE))  # Added na.rm and corrected syntax
  perm[i] <- filter(summary, sex == "F") |>
    pull(mean) -
    filter(summary, sex == "M") |> pull(mean)
}
hist(perm)
```


